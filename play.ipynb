{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6960c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load model and inspect state dict\n",
    "hf_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # 124M\n",
    "hf_state_dict = hf_model.state_dict()\n",
    "\n",
    "for name, tensor in hf_state_dict.items():\n",
    "    print(name, tensor.shape)\n",
    "\n",
    "# Peek at first 20 values of the positional embedding vector (flattened)\n",
    "hf_state_dict[\"transformer.wpe.weight\"].view(-1)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize positional embeddings and a few rows\n",
    "plt.imshow(hf_state_dict[\"transformer.wpe.weight\"], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hf_state_dict[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(hf_state_dict[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(hf_state_dict[\"transformer.wpe.weight\"][:, 250])\n",
    "plt.show()\n",
    "\n",
    "# Visualize a slice of attention projection weights\n",
    "plt.imshow(hf_state_dict[\"transformer.h.1.attn.c_attn.weight\"][:300, :300], cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a59ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "set_seed(42)\n",
    "text_generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7189d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual top-k sampling generation (equivalent to HF defaults but explicit)\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "gen_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # 124M\n",
    "gen_model.eval()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gen_model.to(DEVICE)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Prompt: \"Hello, I'm a language model,\"\n",
    "prompt_tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11]\n",
    "token_batch = torch.tensor(prompt_tokens, dtype=torch.long).unsqueeze(0).repeat(5, 1)  # (B=5, T=8)\n",
    "x_tokens = token_batch.to(DEVICE)\n",
    "\n",
    "max_length = 30\n",
    "top_k = 50\n",
    "\n",
    "with torch.no_grad():\n",
    "    while x_tokens.size(1) < max_length:\n",
    "        logits = gen_model(x_tokens).logits                  # (B, T, vocab_size)\n",
    "        last_logits = logits[:, -1, :]                       # (B, vocab_size)\n",
    "        probs = F.softmax(last_logits, dim=-1)               # (B, vocab_size)\n",
    "        topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)  # (B, top_k)\n",
    "        sampled_idx = torch.multinomial(topk_probs, 1)       # (B, 1)\n",
    "        next_token = torch.gather(topk_indices, -1, sampled_idx)     # (B, 1)\n",
    "        x_tokens = torch.cat((x_tokens, next_token), dim=1)\n",
    "\n",
    "# Decode the first 30 tokens of each sequence\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(x_tokens.size(0)):\n",
    "    seq_tokens = x_tokens[i, :max_length].tolist()\n",
    "    print(\">\", tokenizer.decode(seq_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny Shakespeare sample\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    tiny_text = f.read()\n",
    "\n",
    "data_slice = tiny_text[:1000]  # first 1,000 characters\n",
    "print(data_slice[:100])\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tok_ids = tokenizer.encode(data_slice)\n",
    "print(tok_ids[:24])\n",
    "\n",
    "import torch\n",
    "buf = torch.tensor(tok_ids[:25])  # need 25 to make 4x6 pairs (24) for (x,y)\n",
    "x_win = buf[:-1].view(4, 6)\n",
    "y_win = buf[1:].view(4, 6)\n",
    "print(x_win)\n",
    "print(y_win)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tied weights: lm_head.weight should equal transformer.wte.weight\n",
    "import torch\n",
    "\n",
    "print(hf_state_dict[\"lm_head.weight\"].shape)\n",
    "print(hf_state_dict[\"transformer.wte.weight\"].shape)\n",
    "\n",
    "print(torch.equal(hf_state_dict[\"lm_head.weight\"], hf_state_dict[\"transformer.wte.weight\"]))\n",
    "\n",
    "print(hf_state_dict[\"lm_head.weight\"].data_ptr())\n",
    "print(hf_state_dict[\"transformer.wte.weight\"].data_ptr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ef512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation grows inside the residual stream (toy)\n",
    "import torch\n",
    "\n",
    "residual = torch.zeros(768)\n",
    "num_layers = 100\n",
    "for _ in range(num_layers):\n",
    "    residual += (num_layers ** -0.5) * torch.randn(768)\n",
    "\n",
    "print(residual.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP and gradient check\n",
    "import torch\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(32, 1),\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x_in = torch.randn(4, 16)\n",
    "y_true = torch.randn(4, 1)\n",
    "\n",
    "mlp.zero_grad()\n",
    "y_pred = mlp(x_in)\n",
    "loss = torch.nn.functional.mse_loss(y_pred, y_true)\n",
    "loss.backward()\n",
    "print(mlp[0].weight.grad.view(-1)[:10])\n",
    "\n",
    "# Gradient accumulation demo (restore mean normalizer manually)\n",
    "mlp.zero_grad()\n",
    "for i in range(4):\n",
    "    y_pred_i = mlp(x_in[i])\n",
    "    loss_i = torch.nn.functional.mse_loss(y_pred_i, y_true[i]) / 4.0  # add back 1/4 normalizer\n",
    "    loss_i.backward()\n",
    "print(mlp[0].weight.grad.view(-1)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and visualize a training log\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "size_label = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[size_label]\n",
    "\n",
    "hella_gpt2_baseline = {  # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[size_label]\n",
    "\n",
    "hella_gpt3_baseline = {  # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[size_label]\n",
    "\n",
    "with open(\"log124M_40B/log.txt\", \"r\") as f:\n",
    "    log_lines = f.readlines()\n",
    "\n",
    "# Group metrics by stream (train, val, hella)\n",
    "streams = {}\n",
    "for line in log_lines:\n",
    "    step_str, stream_name, value_str = line.strip().split()\n",
    "    streams.setdefault(stream_name, {})[int(step_str)] = float(value_str)\n",
    "\n",
    "# Convert to sorted x/y for plotting\n",
    "streams_xy = {}\n",
    "for stream_name, kv in streams.items():\n",
    "    xy_sorted = sorted(kv.items())\n",
    "    steps, vals = zip(*xy_sorted)\n",
    "    streams_xy[stream_name] = (list(steps), list(vals))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: train/val losses\n",
    "plt.subplot(1, 2, 1)\n",
    "xs, ys = streams_xy[\"train\"]\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({size_label}) train loss\")\n",
    "print(\"Min Train Loss:\", np.min(ys))\n",
    "\n",
    "xs, ys = streams_xy[\"val\"]\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({size_label}) val loss\")\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, linestyle=\"--\", label=f\"OpenAI GPT-2 ({size_label}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "print(\"Min Validation Loss:\", np.min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(1, 2, 2)\n",
    "xs, ys = streams_xy[\"hella\"]\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({size_label})\")\n",
    "if hella_gpt2_baseline:\n",
    "    plt.axhline(y=hella_gpt2_baseline, linestyle=\"--\", label=f\"OpenAI GPT-2 ({size_label}) checkpoint\")\n",
    "if hella_gpt3_baseline:\n",
    "    plt.axhline(y=hella_gpt3_baseline, linestyle=\"--\", label=f\"OpenAI GPT-3 ({size_label}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "\n",
    "print(\"Max Hellaswag eval:\", np.max(ys))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
